# Frozone Meeting 9/28/2025 (On Zoom, with Noah)

## News

* SD has pitched the experiment to the other DATA 101 / CPSC 110 instructors,
  and has heard one "yes" and zero "nos" so far. He'll keep you updated.

* BH: sum up the response from Marshall, and our current status with the GC
  instance, getting permission from the UMW people, etc.


## Defining unproductive dialogue

Go through issue [https://github.com/divilian/frozone/issues/41](issue #41)
responses.


## Transitioning erratically into a code phase

### Rules of the game

1. I want to make sure that everybody gets a chance to contribute code, no
   matter what other stuff they're also doing for the project. (Just a note for
   future task assignments.)

1. All of us must all be able to run all code pushed to the `main` branch,
   always. Any time this is not true, **it's an immediate show-stopper** until
   resolved. (This is one of the few decisions Stephen will insist on this
   semester.)

1. Get your hands dirty!! With every model you download and get running, or any
   code you write, be sure to _spend enough time playing with it yourself_ to
   get a good sense of what its behavior and limitations are.

1. My guess is that many/all of us will be usually/always using the GC instance
   to do development. This is not a hard requirement; if there are reasons
   you'd like to use your own machine, that's okay too. But we do all need
   accounts there and be able to install and run the project from there.

1. Our goal is more proof-of-concept than long-term maintenance, so occasional
   shortcuts are okay. But we should still all try to write modular,
   encapsulated, inter-operable code. (`.py` files, clean and consistent
   function/class definitions, minimal but good docs)

1. We all need to be on the right version of Python and have all packages and
   versions listed in `requirements.txt`.


### Taking stock of stuff so far

#### SD: the HF playground code

SD demos what he means by "evaluating by playing with it"

#### GM: detector

Demos


## Survey questions

* LR's Survey Questions doc (all three sheets), including:

    * How much are we willing to let the subject be "in on it?" Asking
      pre-survey questions about toxicity may prime them unwisely. General
      question: should we move some of these questions to the post-survey? (LR
      and SD have issue #57 and will seek some Psyc counsel.)

    * The "natural environment" of the experimental participant. Clearly, they
      aren't used to using the lab computers. Are they used to using a
      laptop/desktop at all, or do they only use a phone for social media?


## Zotero "happy chaos"

Let's start doing some more organizing. (Reminder: items can and should
sometimes be in multiple collections.)

* Type of result
    * datasets (e.g., toxicity/Poudhar 2024)
    * models (e.g., toxicity/Caselli 2021)
    * user studies (e.g., toxicity/Hangartner 2021)
    * applications (e.g., tools/Argyle 2023)

* trim some things we've discovered are far afield?

* ...?

* Do we like a separate `datasets.md` file, or should this information just be
  in Zotero?


## Open questions

1. Will the experimental chat room be a round-robin? A response to individuals?
    A free-for-all?

1. Should the detectors operate on individual posts? Or most recent _k_ posts?
    Or something else?

1. Once we have a mix of sources that do (1) echo chamber-y discussions, and
    (2) heated back-and-forths, (3) polite interaction, what percentage of each
    do we include?


## Decisions

1. We'll train bots with all data (even toxic) and self-censor them "on the way
   out" (rather than withholding toxic data during training).


## Plan of record

Action items:

* In light of our conclusions on issue #41, get a handle for how much of each
  aspect is do-able with existing datasets/models and how much we'd have to do
  from scratch.
